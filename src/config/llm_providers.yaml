GEMINI_1.5_FLASH: &gemini_base
  model: gemini-1.5-flash
  temperature: 0.0
  max_output_tokens: 8192

GEMINI_2.0_FLASH:
  <<: *gemini_base
  model: gemini-2.0-flash-exp

OLLAMA_LLAMA3.2:
  host: "http://localhost:11434"
  model: "llama3.2"
  options:
    temperature: 0.5
    top_k: 40
  num_predict: 8192
  stream: False
