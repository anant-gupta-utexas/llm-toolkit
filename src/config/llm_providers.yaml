GEMINI_1.5_FLASH: &gemini_base
  model: gemini-1.5-flash
  temperature: 0.0
  max_output_tokens: 8192

GEMINI_2.0_FLASH:
  <<: *gemini_base
  model: gemini-2.0-flash-exp

OLLAMA_LLAMA3 :
  host: "http://localhost:11434"
  model: "llama3"
  temperature: 0.8
  num_predict: 128
